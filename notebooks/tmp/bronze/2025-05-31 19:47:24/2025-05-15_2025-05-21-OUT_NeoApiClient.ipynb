{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51247d9c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [7]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8dfe58-c663-498d-b069-81e5ebd628b9",
   "metadata": {
    "papermill": {
     "duration": 0.003384,
     "end_time": "2025-06-01T00:47:26.322477",
     "exception": false,
     "start_time": "2025-06-01T00:47:26.319093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\\* Necessary to import from src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d69fe4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T00:47:26.332071Z",
     "iopub.status.busy": "2025-06-01T00:47:26.331122Z",
     "iopub.status.idle": "2025-06-01T00:47:26.338335Z",
     "shell.execute_reply": "2025-06-01T00:47:26.337286Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.011164,
     "end_time": "2025-06-01T00:47:26.339896",
     "exception": false,
     "start_time": "2025-06-01T00:47:26.328732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to Python path\n",
    "parent_dir = Path().resolve().parent\n",
    "sys.path.insert(0, str(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8aa6334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T00:47:26.344832Z",
     "iopub.status.busy": "2025-06-01T00:47:26.344606Z",
     "iopub.status.idle": "2025-06-01T00:47:26.777682Z",
     "shell.execute_reply": "2025-06-01T00:47:26.777236Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.436767,
     "end_time": "2025-06-01T00:47:26.778536",
     "exception": false,
     "start_time": "2025-06-01T00:47:26.341769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "from collections import deque\n",
    "from io import BytesIO\n",
    "from typing import Self, Optional\n",
    "import logging\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from src.transformations import *\n",
    "from src.config import *\n",
    "from src.queue_manager import queue\n",
    "from src.minio_client import *\n",
    "from src.custom_exceptions import *\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='NeoPipeline.log', level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a105f68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T00:47:26.782274Z",
     "iopub.status.busy": "2025-06-01T00:47:26.781893Z",
     "iopub.status.idle": "2025-06-01T00:47:26.784915Z",
     "shell.execute_reply": "2025-06-01T00:47:26.784327Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.006048,
     "end_time": "2025-06-01T00:47:26.785888",
     "exception": false,
     "start_time": "2025-06-01T00:47:26.779840",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Default parameters or parameters set by Airflow\n",
    "api_key_param = NASA_NEO_API_KEY\n",
    "api_uri_param = NASA_NEO_URI\n",
    "start_date_param = '2025-05-02'\n",
    "end_date_param = '2025-05-09'\n",
    "bucket_name_param = 'neo'\n",
    "mode = 'silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19b8703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T00:47:26.789437Z",
     "iopub.status.busy": "2025-06-01T00:47:26.789219Z",
     "iopub.status.idle": "2025-06-01T00:47:26.791912Z",
     "shell.execute_reply": "2025-06-01T00:47:26.791549Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.005204,
     "end_time": "2025-06-01T00:47:26.792700",
     "exception": false,
     "start_time": "2025-06-01T00:47:26.787496",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "api_key_param = \"Sfn0wfG6FG6E3D5Hu8MrxSja38yMXftWqboKv6ZH\"\n",
    "api_uri_param = \"https://api.nasa.gov/neo/rest/v1/feed?\"\n",
    "start_date_param = \"2025-05-15\"\n",
    "end_date_param = \"2025-05-21\"\n",
    "bucket_name_param = \"neo\"\n",
    "mode = \"bronze\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0808ac73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T00:47:26.796258Z",
     "iopub.status.busy": "2025-06-01T00:47:26.796051Z",
     "iopub.status.idle": "2025-06-01T00:47:26.800899Z",
     "shell.execute_reply": "2025-06-01T00:47:26.800579Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.007484,
     "end_time": "2025-06-01T00:47:26.801648",
     "exception": false,
     "start_time": "2025-06-01T00:47:26.794164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to Minio blob storage    \n",
    "minio_client = create_minio_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e79434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T00:47:26.805729Z",
     "iopub.status.busy": "2025-06-01T00:47:26.805473Z",
     "iopub.status.idle": "2025-06-01T00:47:26.829778Z",
     "shell.execute_reply": "2025-06-01T00:47:26.829168Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.027707,
     "end_time": "2025-06-01T00:47:26.830897",
     "exception": false,
     "start_time": "2025-06-01T00:47:26.803190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeoApiClient:\n",
    "    # Instantiate current task API client\n",
    "    def __init__(self, \n",
    "                 api_key,\n",
    "                 api_uri,\n",
    "                 start_date, \n",
    "                 end_date, \n",
    "                 storage, \n",
    "                 bucket_name,\n",
    "                 mode):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize NASA NEO API client.\n",
    "        \n",
    "        Args:\n",
    "            api_key: NASA API authentication key\n",
    "            api_uri: Base URI for NASA NEO API  \n",
    "            start_date: Query start date (YYYY-MM-DD format)\n",
    "            end_date: Query end date (YYYY-MM-DD format)\n",
    "            storage: MinIO client instance for data storage\n",
    "            bucket_name: Target storage bucket name\n",
    "            mode: Data processing mode ('bronze', 'silver', 'gold')\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If date range or mode parameters are invalid\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.debug(f\"Initializing NeoApiClient for date range {start_date} to {end_date}\")\n",
    "        \n",
    "        # Validate inputs\n",
    "        self._validate_inputs(start_date, end_date, mode)\n",
    "        \n",
    "        # set attributes\n",
    "        self.key = api_key\n",
    "        self.api_uri = api_uri\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.storage = storage\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Initialize state\n",
    "        self.data: Optional[dict | pyspark.sql.DataFrame] = None\n",
    "        self.spark: Optional[SparkSession] = None\n",
    "        \n",
    "        logger.debug(f\"NeoApiClient initialized for bucket '{bucket_name}' in {mode} mode\")\n",
    "        \n",
    "    # ================================================================================================\n",
    "    # ATTRIBUTE VALIDATION METHODS\n",
    "    # ================================================================================================\n",
    "        \n",
    "    def _validate_inputs(self, start_date: str, end_date: str, mode: str) -> None:\n",
    "        \"\"\"Validate constructor parameters.\"\"\"\n",
    "\n",
    "        # Validate mode\n",
    "        valid_modes = {'bronze', 'silver', 'gold'}\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(f\"Mode must be one of {valid_modes}, got: {mode}\")\n",
    "\n",
    "        # Valid date values\n",
    "        # can be included in the future\n",
    "\n",
    "    # ================================================================================================\n",
    "    # QUEUE MANAGEMENT METHODS\n",
    "    # ================================================================================================\n",
    "    \n",
    "    def _update_queue(self, mode: str, object_name: Optional[str] = None) -> Optional[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        Update a persistent queue stored in object storage..\n",
    "\n",
    "        Args:\n",
    "            mode: Operation mode - 'in' to add item, 'out' to remove item\n",
    "            object_name: Item to add to queue (required for 'in' mode)\n",
    "\n",
    "        Returns:\n",
    "            str: Item removed from queue (for 'out' mode)\n",
    "            None: For 'in' mode or when queue is empty\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If mode is invalid or object_name missing for 'in' mode\n",
    "            QueueOperationError: If storage operations fail\n",
    "        \"\"\"\n",
    "            \n",
    "        # Validate inputs\n",
    "        if mode not in {'in', 'out'}:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be 'in' or 'out'\")\n",
    "\n",
    "        if mode == 'in' and object_name is None:\n",
    "            raise ValueError(\"object_name is required for 'in' mode\")\n",
    "\n",
    "        logger.debug(f\"Queue operation: mode={mode}, object_name={object_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Select update mode\n",
    "            match mode:\n",
    "                case 'in':\n",
    "                    queue.add_to_queue(item=object_name)\n",
    "\n",
    "                case 'out':\n",
    "                    item = queue.get_from_queue()\n",
    "\n",
    "                    return item\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Queue operation failed: mode={mode}, error={e}\")\n",
    "            raise QueueOperationError(f\"Failed to {mode} queue: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # SPARK SESSION MANAGEMENT METHODS\n",
    "    # ================================================================================================\n",
    "     \n",
    "    def create_spark_session(self, mode: Optional[str] = None) -> SparkSession:\n",
    "        \"\"\"\n",
    "        Create a Spark session configured for Iceberg and MinIO S3A integration.\n",
    "\n",
    "        Configures Spark with the necessary JARs and settings for:\n",
    "        - Apache Iceberg table format support\n",
    "        - MinIO S3-compatible object storage access\n",
    "        - Hadoop S3A filesystem integration\n",
    "\n",
    "        Args:\n",
    "            mode: Processing mode ('bronze', 'silver', 'gold'). \n",
    "                  If None, uses self.mode\n",
    "\n",
    "        Returns:\n",
    "            SparkSession: Configured Spark session ready for data processing\n",
    "\n",
    "        Raises:\n",
    "            SparkSessionError: If session creation fails\n",
    "            FileNotFoundError: If required JAR files are missing\n",
    "            ValueError: If mode is invalid\n",
    "\n",
    "        Example:\n",
    "            spark = client.create_spark_session('bronze')\n",
    "            df = spark.read.table('bronze_catalog.neo_data')\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate and set mode\n",
    "        mode = self._validate_and_set_mode(mode)\n",
    "\n",
    "        # Build configuration\n",
    "        config = self._build_spark_config(mode)\n",
    "\n",
    "        # Create session with error handling\n",
    "        return self._create_session_with_retry(config, mode)\n",
    "\n",
    "    \n",
    "    def _validate_and_set_mode(self, mode: Optional[str]) -> str:\n",
    "        \"\"\"Validate and return the processing mode.\"\"\"\n",
    "        if not mode:\n",
    "            mode = self.mode\n",
    "            logger.debug(f\"Using default mode: {mode}\")\n",
    "\n",
    "        valid_modes = {'bronze', 'silver', 'gold'}\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be one of {valid_modes}\")\n",
    "\n",
    "        logger.debug(f\"Using processing mode: {mode}\")\n",
    "        return mode\n",
    "\n",
    "    \n",
    "    def _build_spark_config(self, mode: str) -> dict:\n",
    "        \"\"\"Build Spark configuration dictionary.\"\"\"\n",
    "        logger.debug(f\"Building Spark configuration for mode: {mode}\")\n",
    "\n",
    "        import pyspark\n",
    "        print(pyspark.__version__)\n",
    "        config = {\n",
    "            # Application\n",
    "            \"spark.app.name\": f\"NASA_NEO_{mode.capitalize()}_Data\",\n",
    "\n",
    "            # Iceberg configuration\n",
    "            \"spark.jars.packages\": \",\".join([\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\",\n",
    "                                             # \"org.apache.iceberg:iceberg-core:1.9.1\",\n",
    "                                             \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "                                             \"org.apache.hadoop:hadoop-common:3.3.4\",\n",
    "                                             \"com.amazonaws:aws-java-sdk-bundle:1.12.367\",\n",
    "                                             # \"org.apache.iceberg:iceberg-aws-bundle:1.4.3\",\n",
    "                                             # \"software.amazon.awssdk:bundle:2.20.18\"\n",
    "                                            ]),\n",
    "            \n",
    "            \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "            \"spark.sql.catalog.iceberg_catalog\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "            \"spark.sql.catalog.iceberg_catalog.type\": \"hadoop\",\n",
    "            \"spark.sql.catalog.iceberg_catalog.warehouse\": f\"s3a://{self.bucket_name}\",\n",
    "            \"spark.sql.catalog.iceberg_catalog.io-impl\": \"org.apache.iceberg.hadoop.HadoopFileIO\",\n",
    "            # \"spark.sql.catalog.iceberg_catalog.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "            \"spark.sql.catalog.iceberg_catalog.s3.endpoint\": f\"http://{MINIO_ENDPOINT}\",\n",
    "            # \"spark.sql.catalog.iceberg_catalog.s3.region\": \"us-east-1\",\n",
    "            # \"spark.sql.defaultCatalog\": \"iceberg_catalog\",\n",
    "            \n",
    "            # Hadoop S3a configuration\n",
    "            \"spark.hadoop.fs.s3a.endpoint\": f\"http://{MINIO_ENDPOINT}\",\n",
    "            \"spark.hadoop.fs.s3a.access.key\": MINIO_ROOT_USER,\n",
    "            \"spark.hadoop.fs.s3a.secret.key\": MINIO_ROOT_PASSWORD,\n",
    "            \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "            \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "            \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "            \"spark.hadoop.fs.s3a.endpoint.region\": \"us-east-1\",\n",
    "            \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "\n",
    "            # Performance Configuration\n",
    "            \"spark.sql.adaptive.enabled\": \"false\",\n",
    "            \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\": \"false\",\n",
    "            \"spark.sql.adaptive.skewJoin.enabled\": \"false\"\n",
    "        }\n",
    "\n",
    "            \n",
    "        logger.debug(f\"Built configuration with {len(config)} settings\")\n",
    "        return config\n",
    "\n",
    "    \n",
    "    def _create_session_with_retry(self, config: dict, mode: str, max_retries: int = 3) -> SparkSession:\n",
    "        \"\"\"Create Spark session with retry logic and proper error handling.\"\"\"\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "\n",
    "                # Build new session\n",
    "                builder = SparkSession.builder\n",
    "\n",
    "                # Apply all configuration\n",
    "                for key, value in config.items():\n",
    "                    builder = builder.config(key, value)\n",
    "\n",
    "                # Create session\n",
    "                spark = builder.getOrCreate()\n",
    "\n",
    "                # Validate session\n",
    "                self._validate_session(spark, mode)\n",
    "\n",
    "                # Save session\n",
    "                self.spark = spark\n",
    "\n",
    "                logger.info(f\"Successfully created Spark session: {spark.sparkContext.appName}\")\n",
    "                \n",
    "                return spark\n",
    "\n",
    "            except Exception as e:\n",
    "                attempt_num = attempt + 1\n",
    "                logger.warning(f\"Spark session creation attempt {attempt_num}/{max_retries} failed: {e}\")\n",
    "\n",
    "                if attempt_num == max_retries:\n",
    "                    logger.error(f\"Failed to create Spark session after {max_retries} attempts\")\n",
    "                    raise SparkSessionError(f\"Could not create Spark session: {e}\")\n",
    "\n",
    "                # Brief pause before retry\n",
    "                import time\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "        \n",
    "    def _validate_session(self, spark: SparkSession, mode: str) -> None:\n",
    "        \"\"\"Validate that the Spark session is working correctly.\"\"\"\n",
    "        try:\n",
    "            # Test basic functionality\n",
    "            spark.sql(\"SELECT 1\").collect()\n",
    "            logger.debug(\"Spark session basic functionality validated\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Spark session validation failed: {e}\")\n",
    "            raise SparkSessionError(f\"Session validation failed: {e}\")\n",
    "\n",
    "            \n",
    "    def close_spark_session(self) -> None:\n",
    "        \"\"\"Properly close the Spark session and clean up resources.\"\"\"\n",
    "        if hasattr(self, 'spark') and self.spark:\n",
    "            try:\n",
    "                logger.info(\"Closing Spark session...\")\n",
    "                self.spark.stop()\n",
    "                self.spark = None\n",
    "                logger.info(\"Spark session closed successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error closing Spark session: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - EXTRACT\n",
    "    # ================================================================================================\n",
    "            \n",
    "    def extract(self) -> Self:\n",
    "        \"\"\"\n",
    "        Extract data based on processing mode.\n",
    "        \n",
    "        Bronze: Fetches from NASA NEO API\n",
    "        Silver: Reads from bronze storage bucket  \n",
    "        Gold: Queries silver Iceberg tables\n",
    "        \n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "            \n",
    "        Raises:\n",
    "            DataExtractionError: When extraction fails for any mode\n",
    "            ValueError: When mode is invalid\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting data extraction for mode: {self.mode}\")\n",
    "       \n",
    "        # Select API Client mode\n",
    "        match self.mode:\n",
    "            case 'bronze':    # Extract from source: NASA NEO API request \n",
    "                self._extract_from_api()\n",
    "            \n",
    "            case 'silver':    # Extract from source: neo/bronze/ \n",
    "                self._extract_from_bronze_storage()\n",
    "                \n",
    "            case 'gold':    # Extract from source: neo/silver/\n",
    "                self._extract_from_silver_tables()\n",
    "                \n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid extraction mode: {self.mode}\")\n",
    "\n",
    "        logger.info(f\"Data extraction completed successfully for mode: {self.mode}\")\n",
    "        return self\n",
    "                \n",
    "                \n",
    "    def _extract_from_api(self) -> None:\n",
    "        \"\"\"Extract raw data from NASA NEO API (Bronze mode).\"\"\"\n",
    "        # Generate API request uri\n",
    "        full_uri = f'{self.api_uri}start_date={self.start_date}&end_date={self.end_date}&api_key={self.key}'\n",
    "        \n",
    "        try:\n",
    "            logger.debug(f\"Making API request to: {full_uri}\")\n",
    "            \n",
    "            response = requests.get(full_uri, timeout=5)\n",
    "            response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "\n",
    "            # Convert JSON to bytes\n",
    "            json_bytes = json.dumps(response.json()).encode('utf-8')\n",
    "\n",
    "            # Store data as BytesIO object\n",
    "            self.data = BytesIO(json_bytes)\n",
    "            logger.info(f\"Successfully extracted {len(json_bytes)} bytes from NASA API\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            raise DataExtractionError(f\"API request timed out\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            raise DataExtractionError(\"Failed to connect to NASA API - check network connection\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            raise DataExtractionError(f\"NASA API returned error: {e.response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise DataExtractionError(f\"API request failed: {e}\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise DataExtractionError(\"NASA API returned invalid JSON\")\n",
    "\n",
    "                    \n",
    "    def _extract_from_bronze_storage(self) -> None:\n",
    "        \"\"\"Extract processed data from bronze storage (Silver mode).\"\"\"\n",
    "        logger.debug(\"Extracting data from bronze storage\")\n",
    "        \n",
    "        # Get next item from queue\n",
    "        obj_name = self._update_queue('out')\n",
    "        \n",
    "        if not obj_name:\n",
    "            logger.info(\"No objects available in queue\")\n",
    "            self.data = None\n",
    "            return\n",
    "        \n",
    "        response = None\n",
    "        try:\n",
    "            logger.debug(f\"Retrieving object: {obj_name}\")\n",
    "            response = self.storage.get_object(self.bucket_name, obj_name)\n",
    "            \n",
    "            # Read and parse JSON data\n",
    "            raw_data = response.data.decode('utf-8')\n",
    "            self.data = json.loads(raw_data)\n",
    "            \n",
    "            logger.info(f\"Successfully extracted object '{obj_name}' from bronze storage\")\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            raise DataExtractionError(f\"Invalid JSON in bronze object: {obj_name}\")\n",
    "        except Exception as e:\n",
    "            # Put item back in queue if extraction failed\n",
    "            if obj_name:\n",
    "                self._update_queue('in', obj_name)\n",
    "            raise DataExtractionError(f\"Failed to extract from bronze storage: {e}\")\n",
    "        finally:\n",
    "            # Always clean up HTTP connection\n",
    "            if response:\n",
    "                response.close()\n",
    "                response.release_conn()\n",
    "         \n",
    "        \n",
    "    def _extract_from_silver_tables(self) -> None:\n",
    "        \"\"\"Extract structured data from silver Iceberg tables (Gold mode).\"\"\"\n",
    "        logger.debug(\"Extracting data from silver Iceberg tables\")\n",
    "        \n",
    "        spark = None\n",
    "        try:\n",
    "            # Create Spark session for silver catalog\n",
    "            spark = self.create_spark_session('silver')\n",
    "            \n",
    "            # Execute query\n",
    "            query = \"SELECT * FROM iceberg_catalog.neo_db.silver_asteroids\"\n",
    "            logger.debug(f\"Executing query: {query}\")\n",
    "            self.data = spark.sql(query)\n",
    "            \n",
    "            # Validate that we got data\n",
    "            if self.data.count() == 0:\n",
    "                logger.warning(\"No data found in silver tables\")\n",
    "            else:\n",
    "                logger.info(f\"Successfully extracted {self.data.count()} records from silver tables\")\n",
    "               \n",
    "        except Exception as e:\n",
    "            raise DataExtractionError(f\"Failed to extract from silver tables: {e}\")\n",
    "\n",
    "        \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - TRANSFORM\n",
    "    # ================================================================================================\n",
    "                     \n",
    "    def transform(self) -> Self:\n",
    "        \"\"\"\n",
    "        Transform data based on processing mode.\n",
    "        \n",
    "        Bronze: Store raw data (no transformation)\n",
    "        Silver: Convert JSON to structured DataFrame\n",
    "        Gold: Prepare analytics-ready dataset\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "\n",
    "        Raises:\n",
    "            DataTransformationError: When transformation fails\n",
    "            ValueError: When mode is invalid\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting data transformation for mode: {self.mode}\")\n",
    "       \n",
    "        try:\n",
    "            match self.mode:\n",
    "                case 'bronze':\n",
    "                    # Bronze mode: no transformation, data already extracted\n",
    "                    logger.info(\"Bronze mode: No transformation required\")\n",
    "\n",
    "                case 'silver':\n",
    "                    # Silver mode: JSON to structured DataFrame\n",
    "                    if not self.data:\n",
    "                        raise DataTransformationError(\"No data available for silver transformation\")\n",
    "\n",
    "                    if not self.spark:\n",
    "                        self.spark = self.create_spark_session('silver')\n",
    "\n",
    "                    self.data = NeoTransformations.json_to_structured_dataframe(self.data, self.spark)\n",
    "\n",
    "                case 'gold':\n",
    "                    # Gold mode: Analytics-ready dataset\n",
    "                    if not self.data:\n",
    "                        raise DataTransformationError(\"No data available for gold transformation\")\n",
    "\n",
    "                    # if not self.spark:\n",
    "                    #     self.spark = self.create_spark_session('gold')   REMOVE\n",
    "\n",
    "                    self.data = NeoTransformations.prepare_analytics_dataset(self.data)\n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid transformation mode: {self.mode}\")\n",
    "\n",
    "            logger.info(f\"Data transformation completed successfully for mode: {self.mode}\")\n",
    "            return self\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data transformation failed for mode {self.mode}: {e}\")\n",
    "            raise DataTransformationError(f\"Transformation failed: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - LOAD\n",
    "    # ================================================================================================\n",
    "                              \n",
    "    def load(self) -> Self:\n",
    "        \"\"\"Load data based on processing mode.\"\"\"\n",
    "        logger.info(f\"Starting data load for mode: {self.mode}\")\n",
    "\n",
    "        try:\n",
    "            match self.mode:\n",
    "                case 'bronze':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for bronze load\")\n",
    "\n",
    "                    obj_name = f'{self.mode}/{self.bucket_name}-{self.start_date}_{self.end_date}.json'\n",
    "\n",
    "                    self.storage.put_object(\n",
    "                        self.bucket_name, \n",
    "                        obj_name, \n",
    "                        self.data,\n",
    "                        length=len(self.data.getvalue()),\n",
    "                        content_type='application/json'\n",
    "                    )\n",
    "\n",
    "                    self._update_queue('in', obj_name)\n",
    "                    logger.info(f\"Successfully stored {obj_name} and added to queue\")\n",
    "\n",
    "                case 'silver':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for silver load\")\n",
    "\n",
    "                    self.spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg_catalog.neo_db\")\n",
    "\n",
    "                    self.data.writeTo(f'iceberg_catalog.neo_db.{self.mode}_asteroids') \\\n",
    "                             .partitionedBy('observation_date') \\\n",
    "                             .createOrReplace()\n",
    "\n",
    "                    logger.info(f\"Successfully loaded data to silver table\")\n",
    "\n",
    "                    self.close_spark_session()\n",
    "\n",
    "                case 'gold':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for gold load\")\n",
    "\n",
    "                    if not self.spark:\n",
    "                        self.spark = self.create_spark_session('gold')\n",
    "\n",
    "                    # self.spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg_catalog.neo_db\") REMOVE\n",
    "\n",
    "                    self.data.writeTo(f'iceberg_catalog.neo_db.{self.mode}_asteroids') \\\n",
    "                             .partitionedBy('observation_date') \\\n",
    "                             .createOrReplace()\n",
    "\n",
    "                    logger.info(f\"Successfully loaded data to gold table\")\n",
    "\n",
    "                    self.close_spark_session()\n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid load mode: {self.mode}\")\n",
    "\n",
    "            logger.info(f\"Data load completed successfully for mode: {self.mode}\")\n",
    "            return self\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data load failed for mode {self.mode}: {e}\")\n",
    "            raise DataLoadError(f\"Load failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c063b",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a21e23d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T00:47:26.835601Z",
     "iopub.status.busy": "2025-06-01T00:47:26.835332Z",
     "iopub.status.idle": "2025-06-01T00:47:32.839435Z",
     "shell.execute_reply": "2025-06-01T00:47:32.838740Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 6.007511,
     "end_time": "2025-06-01T00:47:32.840285",
     "exception": true,
     "start_time": "2025-06-01T00:47:26.832774",
     "status": "failed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "DataExtractionError",
     "evalue": "API request timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='api.nasa.gov', port=443): Read timed out. (read timeout=5)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 324\u001b[39m, in \u001b[36mNeoApiClient._extract_from_api\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    322\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMaking API request to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m response.raise_for_status()  \u001b[38;5;66;03m# Raises HTTPError for bad responses\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/requests/adapters.py:713\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='api.nasa.gov', port=443): Read timed out. (read timeout=5)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mDataExtractionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m neo_client = NeoApiClient(api_key=api_key_param,\n\u001b[32m      3\u001b[39m                           api_uri=api_uri_param,\n\u001b[32m      4\u001b[39m                           start_date=start_date_param, \n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m                           bucket_name=bucket_name_param, \n\u001b[32m      8\u001b[39m                           mode=mode)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Execute ETL pipeline task based on mode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mneo_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.transform().load()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 301\u001b[39m, in \u001b[36mNeoApiClient.extract\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mmatch\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode:\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mbronze\u001b[39m\u001b[33m'\u001b[39m:    \u001b[38;5;66;03m# Extract from source: NASA NEO API request \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_from_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msilver\u001b[39m\u001b[33m'\u001b[39m:    \u001b[38;5;66;03m# Extract from source: neo/bronze/ \u001b[39;00m\n\u001b[32m    304\u001b[39m         \u001b[38;5;28mself\u001b[39m._extract_from_bronze_storage()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 335\u001b[39m, in \u001b[36mNeoApiClient._extract_from_api\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(json_bytes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m bytes from NASA API\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.Timeout:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataExtractionError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request timed out\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.ConnectionError:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataExtractionError(\u001b[33m\"\u001b[39m\u001b[33mFailed to connect to NASA API - check network connection\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mDataExtractionError\u001b[39m: API request timed out"
     ]
    }
   ],
   "source": [
    "# Initialize NeoApiClient\n",
    "neo_client = NeoApiClient(api_key=api_key_param,\n",
    "                          api_uri=api_uri_param,\n",
    "                          start_date=start_date_param, \n",
    "                          end_date=end_date_param, \n",
    "                          storage=minio_client,  \n",
    "                          bucket_name=bucket_name_param, \n",
    "                          mode=mode)\n",
    "\n",
    "# Execute ETL pipeline task based on mode\n",
    "neo_client.extract().transform().load()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nasa_spark_kernel",
   "language": "python",
   "name": "nasa_spark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.186517,
   "end_time": "2025-06-01T00:47:33.461145",
   "environment_variables": {},
   "exception": true,
   "input_path": "/home/fastnnefarious/Projects/PycharmProjects/nasa_neo_pipeline/notebooks/NeoApiClient.ipynb",
   "output_path": "/home/fastnnefarious/Projects/PycharmProjects/nasa_neo_pipeline/notebooks/tmp/bronze/2025-05-31 19:47:24/2025-05-15_2025-05-21-OUT_NeoApiClient.ipynb",
   "parameters": {
    "api_key_param": "Sfn0wfG6FG6E3D5Hu8MrxSja38yMXftWqboKv6ZH",
    "api_uri_param": "https://api.nasa.gov/neo/rest/v1/feed?",
    "bucket_name_param": "neo",
    "end_date_param": "2025-05-21",
    "mode": "bronze",
    "start_date_param": "2025-05-15"
   },
   "start_time": "2025-06-01T00:47:25.274628",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
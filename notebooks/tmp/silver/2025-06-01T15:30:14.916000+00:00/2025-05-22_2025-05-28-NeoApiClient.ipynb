{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec8dfe58-c663-498d-b069-81e5ebd628b9",
   "metadata": {
    "papermill": {
     "duration": 0.002925,
     "end_time": "2025-06-01T15:32:01.080928",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.078003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\\* Necessary to import from src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d69fe4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:32:01.094594Z",
     "iopub.status.busy": "2025-06-01T15:32:01.094429Z",
     "iopub.status.idle": "2025-06-01T15:32:01.098164Z",
     "shell.execute_reply": "2025-06-01T15:32:01.097738Z"
    },
    "papermill": {
     "duration": 0.006369,
     "end_time": "2025-06-01T15:32:01.098753",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.092384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to Python path\n",
    "parent_dir = Path().resolve().parent\n",
    "sys.path.insert(0, str(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8aa6334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:32:01.102508Z",
     "iopub.status.busy": "2025-06-01T15:32:01.102145Z",
     "iopub.status.idle": "2025-06-01T15:32:01.441584Z",
     "shell.execute_reply": "2025-06-01T15:32:01.440973Z"
    },
    "papermill": {
     "duration": 0.342029,
     "end_time": "2025-06-01T15:32:01.442396",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.100367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "from collections import deque\n",
    "from io import BytesIO\n",
    "from typing import Self, Optional\n",
    "import logging\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from src.transformations import *\n",
    "from src.config import *\n",
    "from src.queue_manager import queue\n",
    "from src.minio_client import *\n",
    "from src.custom_exceptions import *\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='NeoPipeline.log', level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a105f68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:32:01.445871Z",
     "iopub.status.busy": "2025-06-01T15:32:01.445642Z",
     "iopub.status.idle": "2025-06-01T15:32:01.448664Z",
     "shell.execute_reply": "2025-06-01T15:32:01.448091Z"
    },
    "papermill": {
     "duration": 0.00592,
     "end_time": "2025-06-01T15:32:01.449662",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.443742",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Default parameters or parameters set by Airflow\n",
    "api_key_param = NASA_NEO_API_KEY\n",
    "api_uri_param = NASA_NEO_URI\n",
    "start_date_param = '2025-05-01'\n",
    "end_date_param = '2025-05-31'\n",
    "bucket_name_param = 'neo'\n",
    "mode = 'silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568a8fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:32:01.452969Z",
     "iopub.status.busy": "2025-06-01T15:32:01.452747Z",
     "iopub.status.idle": "2025-06-01T15:32:01.455187Z",
     "shell.execute_reply": "2025-06-01T15:32:01.454733Z"
    },
    "papermill": {
     "duration": 0.004931,
     "end_time": "2025-06-01T15:32:01.455956",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.451025",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bucket_name_param = \"neo\"\n",
    "mode = \"silver\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796901d1-561b-43bf-863e-4e7d142ff033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:32:01.459030Z",
     "iopub.status.busy": "2025-06-01T15:32:01.458845Z",
     "iopub.status.idle": "2025-06-01T15:32:01.461173Z",
     "shell.execute_reply": "2025-06-01T15:32:01.460704Z"
    },
    "papermill": {
     "duration": 0.004578,
     "end_time": "2025-06-01T15:32:01.461814",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.457236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# li = [('2025-05-01', '2025-05-07'), ('2025-05-08', '2025-05-14'), ('2025-05-15', '2025-05-21'), ('2025-05-22', '2025-05-28'), ('2025-05-29', '2025-05-31')]\n",
    "\n",
    "# for start, end in li:   \n",
    "#     full_uri = f'{NASA_NEO_URI}start_date={start}&end_date={end}&api_key={NASA_NEO_API_KEY}'\n",
    "    \n",
    "#     print(f\"Making API request to: {full_uri}\")\n",
    "    \n",
    "#     response = requests.get(full_uri, timeout=5)\n",
    "#     response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "#     print(f\"HELLO DEVON{response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0808ac73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:32:01.464881Z",
     "iopub.status.busy": "2025-06-01T15:32:01.464696Z",
     "iopub.status.idle": "2025-06-01T15:32:01.469849Z",
     "shell.execute_reply": "2025-06-01T15:32:01.469528Z"
    },
    "papermill": {
     "duration": 0.007343,
     "end_time": "2025-06-01T15:32:01.470502",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.463159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to Minio blob storage    \n",
    "minio_client = create_minio_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e79434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:32:01.473859Z",
     "iopub.status.busy": "2025-06-01T15:32:01.473682Z",
     "iopub.status.idle": "2025-06-01T15:32:01.492856Z",
     "shell.execute_reply": "2025-06-01T15:32:01.492472Z"
    },
    "papermill": {
     "duration": 0.02194,
     "end_time": "2025-06-01T15:32:01.493733",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.471793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeoApiClient:\n",
    "    # Instantiate current task API client\n",
    "    def __init__(self, \n",
    "                 api_key,\n",
    "                 api_uri,\n",
    "                 start_date, \n",
    "                 end_date, \n",
    "                 storage, \n",
    "                 bucket_name,\n",
    "                 mode):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize NASA NEO API client.\n",
    "        \n",
    "        Args:\n",
    "            api_key: NASA API authentication key\n",
    "            api_uri: Base URI for NASA NEO API  \n",
    "            start_date: Query start date (YYYY-MM-DD format)\n",
    "            end_date: Query end date (YYYY-MM-DD format)\n",
    "            storage: MinIO client instance for data storage\n",
    "            bucket_name: Target storage bucket name\n",
    "            mode: Data processing mode ('bronze', 'silver', 'gold')\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If date range or mode parameters are invalid\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.debug(f\"Initializing NeoApiClient for date range {start_date} to {end_date}\")\n",
    "        \n",
    "        # Validate inputs\n",
    "        self._validate_inputs(start_date, end_date, mode)\n",
    "        \n",
    "        # set attributes\n",
    "        self.key = api_key\n",
    "        self.api_uri = api_uri\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.storage = storage\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Initialize state\n",
    "        self.data: Optional[dict | pyspark.sql.DataFrame] = None\n",
    "        self.spark: Optional[SparkSession] = None\n",
    "        \n",
    "        logger.debug(f\"NeoApiClient initialized for bucket '{bucket_name}' in {mode} mode\")\n",
    "        \n",
    "    # ================================================================================================\n",
    "    # ATTRIBUTE VALIDATION METHODS\n",
    "    # ================================================================================================\n",
    "        \n",
    "    def _validate_inputs(self, start_date: str, end_date: str, mode: str) -> None:\n",
    "        \"\"\"Validate constructor parameters.\"\"\"\n",
    "\n",
    "        # Validate mode\n",
    "        valid_modes = {'bronze', 'silver', 'gold'}\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(f\"Mode must be one of {valid_modes}, got: {mode}\")\n",
    "\n",
    "        # Valid date values\n",
    "        # can be included in the future\n",
    "\n",
    "    # ================================================================================================\n",
    "    # QUEUE MANAGEMENT METHODS\n",
    "    # ================================================================================================\n",
    "    \n",
    "    def _update_queue(self, mode: str, object_name: Optional[str] = None) -> Optional[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        Update a persistent queue stored in object storage..\n",
    "\n",
    "        Args:\n",
    "            mode: Operation mode - 'in' to add item, 'out' to remove item\n",
    "            object_name: Item to add to queue (required for 'in' mode)\n",
    "\n",
    "        Returns:\n",
    "            str: Item removed from queue (for 'out' mode)\n",
    "            None: For 'in' mode or when queue is empty\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If mode is invalid or object_name missing for 'in' mode\n",
    "            QueueOperationError: If storage operations fail\n",
    "        \"\"\"\n",
    "            \n",
    "        # Validate inputs\n",
    "        if mode not in {'in', 'out'}:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be 'in' or 'out'\")\n",
    "\n",
    "        if mode == 'in' and object_name is None:\n",
    "            raise ValueError(\"object_name is required for 'in' mode\")\n",
    "\n",
    "        logger.debug(f\"Queue operation: mode={mode}, object_name={object_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Select update mode\n",
    "            match mode:\n",
    "                case 'in':\n",
    "                    queue.add_to_queue(item=object_name)\n",
    "\n",
    "                case 'out':\n",
    "                    item = queue.get_from_queue()\n",
    "\n",
    "                    return item\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Queue operation failed: mode={mode}, error={e}\")\n",
    "            raise QueueOperationError(f\"Failed to {mode} queue: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # SPARK SESSION MANAGEMENT METHODS\n",
    "    # ================================================================================================\n",
    "     \n",
    "    def create_spark_session(self, mode: Optional[str] = None) -> SparkSession:\n",
    "        \"\"\"\n",
    "        Create a Spark session configured for Iceberg and MinIO S3A integration.\n",
    "\n",
    "        Configures Spark with the necessary JARs and settings for:\n",
    "        - Apache Iceberg table format support\n",
    "        - MinIO S3-compatible object storage access\n",
    "        - Hadoop S3A filesystem integration\n",
    "\n",
    "        Args:\n",
    "            mode: Processing mode ('bronze', 'silver', 'gold'). \n",
    "                  If None, uses self.mode\n",
    "\n",
    "        Returns:\n",
    "            SparkSession: Configured Spark session ready for data processing\n",
    "\n",
    "        Raises:\n",
    "            SparkSessionError: If session creation fails\n",
    "            FileNotFoundError: If required JAR files are missing\n",
    "            ValueError: If mode is invalid\n",
    "\n",
    "        Example:\n",
    "            spark = client.create_spark_session('bronze')\n",
    "            df = spark.read.table('bronze_catalog.neo_data')\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate and set mode\n",
    "        mode = self._validate_and_set_mode(mode)\n",
    "\n",
    "        # Build configuration\n",
    "        config = self._build_spark_config(mode)\n",
    "\n",
    "        # Create session with error handling\n",
    "        return self._create_session_with_retry(config, mode)\n",
    "\n",
    "    \n",
    "    def _validate_and_set_mode(self, mode: Optional[str]) -> str:\n",
    "        \"\"\"Validate and return the processing mode.\"\"\"\n",
    "        if not mode:\n",
    "            mode = self.mode\n",
    "            logger.debug(f\"Using default mode: {mode}\")\n",
    "\n",
    "        valid_modes = {'bronze', 'silver', 'gold'}\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be one of {valid_modes}\")\n",
    "\n",
    "        logger.debug(f\"Using processing mode: {mode}\")\n",
    "        return mode\n",
    "\n",
    "    \n",
    "    def _build_spark_config(self, mode: str) -> dict:\n",
    "        \"\"\"Build Spark configuration dictionary.\"\"\"\n",
    "        logger.debug(f\"Building Spark configuration for mode: {mode}\")\n",
    "\n",
    "        import pyspark\n",
    "        print(pyspark.__version__)\n",
    "        config = {\n",
    "            # Application\n",
    "            \"spark.app.name\": f\"NASA_NEO_{mode.capitalize()}_Data\",\n",
    "\n",
    "            # Iceberg configuration\n",
    "            \"spark.jars.packages\": \",\".join([\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\",\n",
    "                                             \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "                                             \"org.apache.hadoop:hadoop-common:3.3.4\",\n",
    "                                             \"com.amazonaws:aws-java-sdk-bundle:1.12.367\",\n",
    "                                            ]),\n",
    "            \n",
    "            \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "            \"spark.sql.catalog.iceberg_catalog\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "            \"spark.sql.catalog.iceberg_catalog.type\": \"hadoop\",\n",
    "            \"spark.sql.catalog.iceberg_catalog.warehouse\": f\"s3a://{self.bucket_name}\",\n",
    "            \"spark.sql.catalog.iceberg_catalog.io-impl\": \"org.apache.iceberg.hadoop.HadoopFileIO\",\n",
    "            \"spark.sql.catalog.iceberg_catalog.s3.endpoint\": f\"http://{MINIO_ENDPOINT}\",\n",
    "            \n",
    "            # Hadoop S3a configuration\n",
    "            \"spark.hadoop.fs.s3a.endpoint\": f\"http://{MINIO_ENDPOINT}\",\n",
    "            \"spark.hadoop.fs.s3a.access.key\": MINIO_ROOT_USER,\n",
    "            \"spark.hadoop.fs.s3a.secret.key\": MINIO_ROOT_PASSWORD,\n",
    "            \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "            \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "            \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "            \"spark.hadoop.fs.s3a.endpoint.region\": \"us-east-1\",\n",
    "            \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "\n",
    "            # Performance Configuration\n",
    "            \"spark.sql.adaptive.enabled\": \"false\",\n",
    "            \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.adaptive.coalescePartitions.enabled\": \"false\",\n",
    "            \"spark.sql.adaptive.skewJoin.enabled\": \"false\"\n",
    "        }\n",
    "\n",
    "            \n",
    "        logger.debug(f\"Built configuration with {len(config)} settings\")\n",
    "        return config\n",
    "\n",
    "    \n",
    "    def _create_session_with_retry(self, config: dict, mode: str, max_retries: int = 3) -> SparkSession:\n",
    "        \"\"\"Create Spark session with retry logic and proper error handling.\"\"\"\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "\n",
    "                # Build new session\n",
    "                builder = SparkSession.builder\n",
    "\n",
    "                # Apply all configuration\n",
    "                for key, value in config.items():\n",
    "                    builder = builder.config(key, value)\n",
    "\n",
    "                # Create session\n",
    "                spark = builder.getOrCreate()\n",
    "\n",
    "                # Validate session\n",
    "                self._validate_session(spark, mode)\n",
    "\n",
    "                # Save session\n",
    "                self.spark = spark\n",
    "\n",
    "                logger.info(f\"Successfully created Spark session: {spark.sparkContext.appName}\")\n",
    "                \n",
    "                return spark\n",
    "\n",
    "            except Exception as e:\n",
    "                attempt_num = attempt + 1\n",
    "                logger.warning(f\"Spark session creation attempt {attempt_num}/{max_retries} failed: {e}\")\n",
    "\n",
    "                if attempt_num == max_retries:\n",
    "                    logger.error(f\"Failed to create Spark session after {max_retries} attempts\")\n",
    "                    raise SparkSessionError(f\"Could not create Spark session: {e}\")\n",
    "\n",
    "                # Brief pause before retry\n",
    "                import time\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "        \n",
    "    def _validate_session(self, spark: SparkSession, mode: str) -> None:\n",
    "        \"\"\"Validate that the Spark session is working correctly.\"\"\"\n",
    "        try:\n",
    "            # Test basic functionality\n",
    "            spark.sql(\"SELECT 1\").collect()\n",
    "            logger.debug(\"Spark session basic functionality validated\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Spark session validation failed: {e}\")\n",
    "            raise SparkSessionError(f\"Session validation failed: {e}\")\n",
    "\n",
    "            \n",
    "    def close_spark_session(self) -> None:\n",
    "        \"\"\"Properly close the Spark session and clean up resources.\"\"\"\n",
    "        if hasattr(self, 'spark') and self.spark:\n",
    "            try:\n",
    "                logger.info(\"Closing Spark session...\")\n",
    "                self.spark.stop()\n",
    "                self.spark = None\n",
    "                logger.info(\"Spark session closed successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error closing Spark session: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - EXTRACT\n",
    "    # ================================================================================================\n",
    "            \n",
    "    def extract(self) -> Self:\n",
    "        \"\"\"\n",
    "        Extract data based on processing mode.\n",
    "        \n",
    "        Bronze: Fetches from NASA NEO API\n",
    "        Silver: Reads from bronze storage bucket  \n",
    "        Gold: Queries silver Iceberg tables\n",
    "        \n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "            \n",
    "        Raises:\n",
    "            DataExtractionError: When extraction fails for any mode\n",
    "            ValueError: When mode is invalid\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting data extraction for mode: {self.mode}\")\n",
    "       \n",
    "        # Select API Client mode\n",
    "        match self.mode:\n",
    "            case 'bronze':    # Extract from source: NASA NEO API request \n",
    "                self._extract_from_api()\n",
    "            \n",
    "            case 'silver':    # Extract from source: neo/bronze/ \n",
    "                self._extract_from_bronze_storage()\n",
    "                \n",
    "            case 'gold':    # Extract from source: neo/silver/\n",
    "                self._extract_from_silver_tables()\n",
    "                \n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid extraction mode: {self.mode}\")\n",
    "\n",
    "        logger.info(f\"Data extraction completed successfully for mode: {self.mode}\")\n",
    "        return self\n",
    "                \n",
    "                \n",
    "    def _extract_from_api(self) -> None:\n",
    "        \"\"\"Extract raw data from NASA NEO API (Bronze mode).\"\"\"\n",
    "        # Generate API request uri\n",
    "        full_uri = f'{self.api_uri}start_date={self.start_date}&end_date={self.end_date}&api_key={self.key}'\n",
    "        \n",
    "        try:\n",
    "            logger.debug(f\"Making API request to: {full_uri}\")\n",
    "            \n",
    "            response = requests.get(full_uri, timeout=5)\n",
    "            response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "\n",
    "            # Convert JSON to bytes\n",
    "            json_bytes = json.dumps(response.json()).encode('utf-8')\n",
    "\n",
    "            # Store data as BytesIO object\n",
    "            self.data = BytesIO(json_bytes)\n",
    "            logger.info(f\"Successfully extracted {len(json_bytes)} bytes from NASA API\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            raise DataExtractionError(f\"API request timed out\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            raise DataExtractionError(\"Failed to connect to NASA API - check network connection\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            raise DataExtractionError(f\"NASA API returned error: {e.response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise DataExtractionError(f\"API request failed: {e}\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise DataExtractionError(\"NASA API returned invalid JSON\")\n",
    "\n",
    "                    \n",
    "    def _extract_from_bronze_storage(self) -> None:\n",
    "        \"\"\"Extract processed data from bronze storage (Silver mode).\"\"\"\n",
    "        logger.debug(\"Extracting data from bronze storage\")\n",
    "        \n",
    "        # Get next item from queue\n",
    "        obj_name = self._update_queue('out')\n",
    "        \n",
    "        if not obj_name:\n",
    "            logger.info(\"No objects available in queue\")\n",
    "            self.data = None\n",
    "            return\n",
    "        \n",
    "        response = None\n",
    "        try:\n",
    "            logger.debug(f\"Retrieving object: {obj_name}\")\n",
    "            response = self.storage.get_object(self.bucket_name, obj_name)\n",
    "            \n",
    "            # Read and parse JSON data\n",
    "            raw_data = response.data.decode('utf-8')\n",
    "            self.data = json.loads(raw_data)\n",
    "            \n",
    "            logger.info(f\"Successfully extracted object '{obj_name}' from bronze storage\")\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            raise DataExtractionError(f\"Invalid JSON in bronze object: {obj_name}\")\n",
    "        except Exception as e:\n",
    "            # Put item back in queue if extraction failed\n",
    "            if obj_name:\n",
    "                self._update_queue('in', obj_name)\n",
    "            raise DataExtractionError(f\"Failed to extract from bronze storage: {e}\")\n",
    "        finally:\n",
    "            # Always clean up HTTP connection\n",
    "            if response:\n",
    "                response.close()\n",
    "                response.release_conn()\n",
    "         \n",
    "        \n",
    "    def _extract_from_silver_tables(self) -> None:\n",
    "        \"\"\"Extract structured data from silver Iceberg tables (Gold mode).\"\"\"\n",
    "        logger.debug(\"Extracting data from silver Iceberg tables\")\n",
    "        \n",
    "        spark = None\n",
    "        try:\n",
    "            # Create Spark session for silver catalog\n",
    "            spark = self.create_spark_session('silver')\n",
    "            \n",
    "            # Execute query\n",
    "            query = \"SELECT * FROM iceberg_catalog.neo_db.silver_asteroids\"\n",
    "            logger.debug(f\"Executing query: {query}\")\n",
    "            self.data = spark.sql(query)\n",
    "            \n",
    "            # Validate that we got data\n",
    "            if self.data.count() == 0:\n",
    "                logger.warning(\"No data found in silver tables\")\n",
    "            else:\n",
    "                logger.info(f\"Successfully extracted {self.data.count()} records from silver tables\")\n",
    "               \n",
    "        except Exception as e:\n",
    "            raise DataExtractionError(f\"Failed to extract from silver tables: {e}\")\n",
    "\n",
    "        \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - TRANSFORM\n",
    "    # ================================================================================================\n",
    "                     \n",
    "    def transform(self) -> Self:\n",
    "        \"\"\"\n",
    "        Transform data based on processing mode.\n",
    "        \n",
    "        Bronze: Store raw data (no transformation)\n",
    "        Silver: Convert JSON to structured DataFrame\n",
    "        Gold: Prepare analytics-ready dataset\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "\n",
    "        Raises:\n",
    "            DataTransformationError: When transformation fails\n",
    "            ValueError: When mode is invalid\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting data transformation for mode: {self.mode}\")\n",
    "       \n",
    "        try:\n",
    "            match self.mode:\n",
    "                case 'bronze':\n",
    "                    # Bronze mode: no transformation, data already extracted\n",
    "                    logger.info(\"Bronze mode: No transformation required\")\n",
    "\n",
    "                case 'silver':\n",
    "                    # Silver mode: JSON to structured DataFrame\n",
    "                    if not self.data:\n",
    "                        raise DataTransformationError(\"No data available for silver transformation\")\n",
    "\n",
    "                    if not self.spark:\n",
    "                        self.spark = self.create_spark_session('silver')\n",
    "\n",
    "                    self.data = NeoTransformations.json_to_structured_dataframe(self.data, self.spark)\n",
    "\n",
    "                case 'gold':\n",
    "                    # Gold mode: Analytics-ready dataset\n",
    "                    if not self.data:\n",
    "                        raise DataTransformationError(\"No data available for gold transformation\")\n",
    "\n",
    "                    self.data = NeoTransformations.prepare_analytics_dataset(self.data)\n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid transformation mode: {self.mode}\")\n",
    "\n",
    "            logger.info(f\"Data transformation completed successfully for mode: {self.mode}\")\n",
    "            return self\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data transformation failed for mode {self.mode}: {e}\")\n",
    "            raise DataTransformationError(f\"Transformation failed: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - LOAD\n",
    "    # ================================================================================================\n",
    "                              \n",
    "    def load(self) -> Self:\n",
    "        \"\"\"Load data based on processing mode.\"\"\"\n",
    "        logger.info(f\"Starting data load for mode: {self.mode}\")\n",
    "\n",
    "        try:\n",
    "            match self.mode:\n",
    "                case 'bronze':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for bronze load\")\n",
    "\n",
    "                    obj_name = f'{self.mode}/{self.bucket_name}-{self.start_date}_{self.end_date}.json'\n",
    "\n",
    "                    self.storage.put_object(\n",
    "                        self.bucket_name, \n",
    "                        obj_name, \n",
    "                        self.data,\n",
    "                        length=len(self.data.getvalue()),\n",
    "                        content_type='application/json'\n",
    "                    )\n",
    "\n",
    "                    self._update_queue('in', obj_name)\n",
    "                    logger.info(f\"Successfully stored {obj_name} and added to queue\")\n",
    "\n",
    "                case 'silver':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for silver load\")\n",
    "\n",
    "                    self.spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg_catalog.neo_db\")\n",
    "\n",
    "                    self.data.writeTo(f'iceberg_catalog.neo_db.{self.mode}_asteroids') \\\n",
    "                             .partitionedBy('observation_date') \\\n",
    "                             .createOrReplace()\n",
    "\n",
    "                    logger.info(f\"Successfully loaded data to silver table\")\n",
    "\n",
    "                    self.close_spark_session()\n",
    "\n",
    "                case 'gold':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for gold load\")\n",
    "\n",
    "                    if not self.spark:\n",
    "                        self.spark = self.create_spark_session('gold')\n",
    "\n",
    "                    self.data.writeTo(f'iceberg_catalog.neo_db.{self.mode}_asteroids') \\\n",
    "                             .partitionedBy('observation_date') \\\n",
    "                             .createOrReplace()\n",
    "\n",
    "                    logger.info(f\"Successfully loaded data to gold table\")\n",
    "\n",
    "                    self.close_spark_session()\n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid load mode: {self.mode}\")\n",
    "\n",
    "            logger.info(f\"Data load completed successfully for mode: {self.mode}\")\n",
    "            return self\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data load failed for mode {self.mode}: {e}\")\n",
    "            raise DataLoadError(f\"Load failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a21e23d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:32:01.496978Z",
     "iopub.status.busy": "2025-06-01T15:32:01.496781Z",
     "iopub.status.idle": "2025-06-01T15:32:18.748120Z",
     "shell.execute_reply": "2025-06-01T15:32:18.746764Z"
    },
    "papermill": {
     "duration": 17.254736,
     "end_time": "2025-06-01T15:32:18.749738",
     "exception": false,
     "start_time": "2025-06-01T15:32:01.495002",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/01 10:32:02 WARN Utils: Your hostname, newlife resolves to a loopback address: 127.0.1.1; using 192.168.0.196 instead (on interface wlp0s20f3)\n",
      "25/06/01 10:32:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/fastnnefarious/.ivy2/cache\n",
      "The jars for the packages stored in: /home/fastnnefarious/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-153409d1-2d6e-4b2e-b19a-db6bef875cc7;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/fastnnefarious/Projects/PycharmProjects/nasa_neo_pipeline/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.367 in central\n",
      ":: resolution report :: resolve 1280ms :: artifacts dl 44ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.367 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.3 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.367] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   97  |   0   |   0   |   1   ||   96  |   0   |\n",
      "\t---------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: retrieving :: org.apache.spark#spark-submit-parent-153409d1-2d6e-4b2e-b19a-db6bef875cc7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 96 already retrieved (0kB/28ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/01 10:32:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "25/06/01 10:32:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/01 10:32:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:============>                                           (45 + 8) / 200]\r",
      "\r",
      "[Stage 4:==================>                                     (65 + 8) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:========================>                               (89 + 8) / 200]\r",
      "\r",
      "[Stage 4:==============================>                        (111 + 8) / 200]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:=======================================>              (147 + 11) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeoApiClient at 0x732f7e062b70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize NeoApiClient\n",
    "neo_client = NeoApiClient(api_key=api_key_param,\n",
    "                          api_uri=api_uri_param,\n",
    "                          start_date=start_date_param, \n",
    "                          end_date=end_date_param, \n",
    "                          storage=minio_client,  \n",
    "                          bucket_name=bucket_name_param, \n",
    "                          mode=mode)\n",
    "\n",
    "# Execute ETL pipeline task based on mode\n",
    "neo_client.extract().transform().load()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nasa_spark_kernel",
   "language": "python",
   "name": "nasa_spark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.960835,
   "end_time": "2025-06-01T15:32:21.371994",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/fastnnefarious/Projects/PycharmProjects/nasa_neo_pipeline/notebooks/NeoApiClient.ipynb",
   "output_path": "/home/fastnnefarious/Projects/PycharmProjects/nasa_neo_pipeline/notebooks/tmp/silver/2025-06-01T15:30:14.916000+00:00/2025-05-22_2025-05-28-NeoApiClient.ipynb",
   "parameters": {
    "bucket_name_param": "neo",
    "mode": "silver"
   },
   "start_time": "2025-06-01T15:32:00.411159",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
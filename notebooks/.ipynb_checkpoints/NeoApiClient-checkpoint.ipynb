{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d69fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to Python path\n",
    "parent_dir = Path().resolve().parent\n",
    "sys.path.insert(0, str(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8aa6334",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqueue_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m queue\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "from collections import deque\n",
    "from io import BytesIO\n",
    "from typing import Self, Optional\n",
    "import logging\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from src.transformations import *\n",
    "from src.config import *\n",
    "from src.queue_manager import queue\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='NeoPipeline.log', level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29735f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Exception classes\n",
    "class SparkSessionError(Exception):\n",
    "    \"\"\"Raised when Spark session operations fail.\"\"\"\n",
    "    pass\n",
    "\n",
    "class QueueOperationError(Exception):\n",
    "    \"\"\"Raised when queue operations fail.\"\"\"\n",
    "    pass\n",
    "\n",
    "class DataExtractionError(Exception):\n",
    "    \"\"\"Raised when data extraction fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "class DataLoadError(Exception):\n",
    "    \"\"\"Raised when data load operations fail.\"\"\"\n",
    "    pass\n",
    "\n",
    "class MinioConnectionError(Exception):\n",
    "    \"\"\"Raised when connection to MinIO fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "class MinioAuthenticationError(Exception):\n",
    "    \"\"\"Raised when MinIO authentication fails.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a105f68",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Default parameters or parameters set by Airflow\n",
    "api_key_param = NASA_NEO_API_KEY\n",
    "api_uri_param = NASA_NEO_URI\n",
    "start_date_param = '2025-05-02'\n",
    "end_date_param = '2025-05-09'\n",
    "bucket_name_param = 'neo'\n",
    "mode = 'silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0808ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Minio blob storage\n",
    "def create_minio_client(endpoint=MINIO_ENDPOINT, \n",
    "                        access_key=MINIO_ROOT_USER, \n",
    "                        secret_key=MINIO_ROOT_PASSWORD, \n",
    "                        secure=False) -> Optional[Minio]:\n",
    "    \"\"\"\n",
    "    Create and validate a MinIO client connection.\n",
    "\n",
    "    Initializes a MinIO API client and verifies connectivity by attempting\n",
    "    to list buckets. This ensures the client is ready for storage operations.\n",
    "\n",
    "    Args:\n",
    "        endpoint (str): MinIO server endpoint (e.g., 'localhost:9000')\n",
    "        access_key (str): Authentication access key/username\n",
    "        secret_key (str): Authentication secret key/password  \n",
    "        secure (bool): Whether to use HTTPS (True) or HTTP (False)\n",
    "\n",
    "    Returns:\n",
    "        Minio: Configured and validated MinIO client instance\n",
    "\n",
    "    Raises:\n",
    "        MinioConnectionError: When client creation or connection validation fails\n",
    "        MinioAuthenticationError: When authentication credentials are invalid\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"Creating MinIO client for endpoint: {endpoint}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize client\n",
    "        client = Minio(\n",
    "            endpoint=endpoint,\n",
    "            access_key=access_key, \n",
    "            secret_key=secret_key,\n",
    "            secure=secure\n",
    "        )\n",
    "        logger.debug(\"MinIO client instance created successfully\")\n",
    "        \n",
    "        # Validate connection\n",
    "        logger.info(\"Validating MinIO connection...\")\n",
    "        client.list_buckets()\n",
    "        logger.info(\"MinIO connection validated successfully\")\n",
    "        \n",
    "        return client\n",
    "        \n",
    "    except S3Error as e:\n",
    "        logger.error(f\"MinIO authentication failed: {e}\")\n",
    "        raise MinioAuthenticationError(f\"Authentication failed for {endpoint}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to MinIO at {endpoint}: {e}\")\n",
    "        raise MinioConnectionError(f\"Connection failed for {endpoint}: {e}\")\n",
    "\n",
    "    \n",
    "minio_client = create_minio_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e79434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeoApiClient:\n",
    "    # Instantiate current task API client\n",
    "    def __init__(self, \n",
    "                 api_key,\n",
    "                 api_uri,\n",
    "                 start_date, \n",
    "                 end_date, \n",
    "                 storage, \n",
    "                 bucket_name,\n",
    "                 mode):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize NASA NEO API client.\n",
    "        \n",
    "        Args:\n",
    "            api_key: NASA API authentication key\n",
    "            api_uri: Base URI for NASA NEO API  \n",
    "            start_date: Query start date (YYYY-MM-DD format)\n",
    "            end_date: Query end date (YYYY-MM-DD format)\n",
    "            storage: MinIO client instance for data storage\n",
    "            bucket_name: Target storage bucket name\n",
    "            mode: Data processing mode ('bronze', 'silver', 'gold')\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If date range or mode parameters are invalid\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.debug(f\"Initializing NeoApiClient for date range {start_date} to {end_date}\")\n",
    "        \n",
    "        # Validate inputs\n",
    "        self._validate_inputs(start_date, end_date, mode)\n",
    "        \n",
    "        # set attributes\n",
    "        self.key = api_key\n",
    "        self.api_uri = api_uri\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.storage = storage\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Initialize state\n",
    "        self.data: Optional[dict | pyspark.sql.DataFrame] = None\n",
    "        self.spark: Optional[SparkSession] = None\n",
    "        \n",
    "        logger.debug(f\"NeoApiClient initialized for bucket '{bucket_name}' in {mode} mode\")\n",
    "        \n",
    "    # ================================================================================================\n",
    "    # ATTRIBUTE VALIDATION METHODS\n",
    "    # ================================================================================================\n",
    "        \n",
    "    def _validate_inputs(self, start_date: str, end_date: str, mode: str) -> None:\n",
    "        \"\"\"Validate constructor parameters.\"\"\"\n",
    "\n",
    "        # Validate mode\n",
    "        valid_modes = {'bronze', 'silver', 'gold'}\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(f\"Mode must be one of {valid_modes}, got: {mode}\")\n",
    "\n",
    "        # Valid date values\n",
    "        # can be included in the future\n",
    "\n",
    "    # ================================================================================================\n",
    "    # QUEUE MANAGEMENT METHODS\n",
    "    # ================================================================================================\n",
    "    \n",
    "    def _update_queue(self, mode: str, object_name: Optional[str] = None) -> Optional[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        Update a persistent queue stored in object storage..\n",
    "\n",
    "        Args:\n",
    "            mode: Operation mode - 'in' to add item, 'out' to remove item\n",
    "            object_name: Item to add to queue (required for 'in' mode)\n",
    "\n",
    "        Returns:\n",
    "            str: Item removed from queue (for 'out' mode)\n",
    "            None: For 'in' mode or when queue is empty\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If mode is invalid or object_name missing for 'in' mode\n",
    "            QueueOperationError: If storage operations fail\n",
    "        \"\"\"\n",
    "            \n",
    "        # Validate inputs\n",
    "        if mode not in {'in', 'out'}:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be 'in' or 'out'\")\n",
    "\n",
    "        if mode == 'in' and object_name is None:\n",
    "            raise ValueError(\"object_name is required for 'in' mode\")\n",
    "\n",
    "        logger.debug(f\"Queue operation: mode={mode}, object_name={object_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Select update mode\n",
    "            match mode:\n",
    "                case 'in':\n",
    "                    queue.add_to_queue(item=object_name)\n",
    "\n",
    "                case 'out':\n",
    "                    item = queue.get_from_queue()\n",
    "\n",
    "                    return item\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Queue operation failed: mode={mode}, error={e}\")\n",
    "            raise QueueOperationError(f\"Failed to {mode} queue: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # SPARK SESSION MANAGEMENT METHODS\n",
    "    # ================================================================================================\n",
    "     \n",
    "    def create_spark_session(self, mode: Optional[str] = None) -> SparkSession:\n",
    "        \"\"\"\n",
    "        Create a Spark session configured for Iceberg and MinIO S3A integration.\n",
    "\n",
    "        Configures Spark with the necessary JARs and settings for:\n",
    "        - Apache Iceberg table format support\n",
    "        - MinIO S3-compatible object storage access\n",
    "        - Hadoop S3A filesystem integration\n",
    "\n",
    "        Args:\n",
    "            mode: Processing mode ('bronze', 'silver', 'gold'). \n",
    "                  If None, uses self.mode\n",
    "\n",
    "        Returns:\n",
    "            SparkSession: Configured Spark session ready for data processing\n",
    "\n",
    "        Raises:\n",
    "            SparkSessionError: If session creation fails\n",
    "            FileNotFoundError: If required JAR files are missing\n",
    "            ValueError: If mode is invalid\n",
    "\n",
    "        Example:\n",
    "            spark = client.create_spark_session('bronze')\n",
    "            df = spark.read.table('bronze_catalog.neo_data')\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate and set mode\n",
    "        mode = self._validate_and_set_mode(mode)\n",
    "\n",
    "        # Check JAR files exist\n",
    "        jar_paths = self._get_and_validate_jar_paths()\n",
    "\n",
    "        # Build configuration\n",
    "        config = self._build_spark_config(mode, jar_paths)\n",
    "\n",
    "        # Create session with error handling\n",
    "        return self._create_session_with_retry(config, mode)\n",
    "\n",
    "    \n",
    "    def _validate_and_set_mode(self, mode: Optional[str]) -> str:\n",
    "        \"\"\"Validate and return the processing mode.\"\"\"\n",
    "        if not mode:\n",
    "            mode = self.mode\n",
    "            logger.debug(f\"Using default mode: {mode}\")\n",
    "\n",
    "        valid_modes = {'bronze', 'silver', 'gold'}\n",
    "        if mode not in valid_modes:\n",
    "            raise ValueError(f\"Invalid mode '{mode}'. Must be one of {valid_modes}\")\n",
    "\n",
    "        logger.debug(f\"Using processing mode: {mode}\")\n",
    "        return mode\n",
    "\n",
    "    \n",
    "    def _get_and_validate_jar_paths(self) -> str:\n",
    "        \"\"\"Get JAR paths and validate they exist.\"\"\"\n",
    "        jar_dir = Path(\"jars\")\n",
    "\n",
    "        required_jars = [\n",
    "            ICEBERG_JAR,\n",
    "            HADOOP_JAR, \n",
    "            AWS_JAVA_SDK_JAR\n",
    "        ]\n",
    "\n",
    "        missing_jars = []\n",
    "        jar_paths = []\n",
    "\n",
    "        for jar in required_jars:\n",
    "            jar_path = jar_dir / jar\n",
    "            if jar_path.exists():\n",
    "                jar_paths.append(str(jar_path))\n",
    "                logger.debug(f\"Found JAR: {jar_path}\")\n",
    "            else:\n",
    "                missing_jars.append(str(jar_path))\n",
    "\n",
    "        if missing_jars:\n",
    "            raise FileNotFoundError(f\"Required JAR files not found: {missing_jars}\")\n",
    "\n",
    "        all_jars = \",\".join(jar_paths)\n",
    "        logger.info(f\"Using JAR files: {all_jars}\")\n",
    "        return all_jars\n",
    "\n",
    "    \n",
    "    def _build_spark_config(self, mode: str, jar_paths: str) -> dict:\n",
    "        \"\"\"Build Spark configuration dictionary.\"\"\"\n",
    "        logger.debug(f\"Building Spark configuration for mode: {mode}\")\n",
    "\n",
    "        config = {\n",
    "            # Application\n",
    "            \"spark.app.name\": f\"NASA_NEO_{mode.capitalize()}_Data\",\n",
    "\n",
    "            # JARs\n",
    "            \"spark.jars\": jar_paths,\n",
    "\n",
    "            # Iceberg Configuration\n",
    "            \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "            f\"spark.sql.catalog.{mode}_catalog\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "            f\"spark.sql.catalog.{mode}_catalog.type\": \"hadoop\",\n",
    "            f\"spark.sql.catalog.{mode}_catalog.warehouse\": f\"s3a://{self.bucket_name}/{mode}\",\n",
    "\n",
    "            # MinIO S3A Configuration  \n",
    "            \"spark.hadoop.fs.s3a.endpoint\": f\"http://{MINIO_ENDPOINT}\",\n",
    "            \"spark.hadoop.fs.s3a.access.key\": MINIO_ROOT_USER,\n",
    "            \"spark.hadoop.fs.s3a.secret.key\": MINIO_ROOT_PASSWORD,\n",
    "            \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "            \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "            \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "            \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "\n",
    "            # Performance Configuration\n",
    "            \"spark.sql.adaptive.enabled\": \"false\",\n",
    "            \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        }\n",
    "\n",
    "        logger.debug(f\"Built configuration with {len(config)} settings\")\n",
    "        return config\n",
    "\n",
    "    \n",
    "    def _create_session_with_retry(self, config: dict, mode: str, max_retries: int = 3) -> SparkSession:\n",
    "        \"\"\"Create Spark session with retry logic and proper error handling.\"\"\"\n",
    "\n",
    "        logger.info(f\"Creating Spark session for mode '{mode}' (attempt 1/{max_retries})\")\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Check if session already exists and can be reused\n",
    "                existing_session = self._get_existing_session(mode)\n",
    "                if existing_session:\n",
    "                    logger.info(f\"Reusing existing Spark session for mode '{mode}'\")\n",
    "                    return existing_session\n",
    "\n",
    "                # Build new session\n",
    "                builder = SparkSession.builder\n",
    "\n",
    "                # Apply all configuration\n",
    "                for key, value in config.items():\n",
    "                    builder = builder.config(key, value)\n",
    "\n",
    "                # Create session\n",
    "                spark = builder.getOrCreate()\n",
    "\n",
    "                # Validate session\n",
    "                self._validate_session(spark, mode)\n",
    "\n",
    "                # Save session\n",
    "                self.spark = spark\n",
    "\n",
    "                logger.info(f\"Successfully created Spark session: {spark.sparkContext.appName}\")\n",
    "                \n",
    "                return spark\n",
    "\n",
    "            except Exception as e:\n",
    "                attempt_num = attempt + 1\n",
    "                logger.warning(f\"Spark session creation attempt {attempt_num}/{max_retries} failed: {e}\")\n",
    "\n",
    "                if attempt_num == max_retries:\n",
    "                    logger.error(f\"Failed to create Spark session after {max_retries} attempts\")\n",
    "                    raise SparkSessionError(f\"Could not create Spark session: {e}\")\n",
    "\n",
    "                # Brief pause before retry\n",
    "                import time\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "                \n",
    "    def _get_existing_session(self, mode: str) -> Optional[SparkSession]:\n",
    "        \"\"\"Check if compatible existing session can be reused.\"\"\"\n",
    "        try:\n",
    "            current_session = SparkSession.getActiveSession()\n",
    "            if current_session and not current_session.sparkContext._jsc.sc().isStopped():\n",
    "                # Check if session is compatible with current mode\n",
    "                app_name = current_session.sparkContext.appName\n",
    "                if mode.capitalize() in app_name:\n",
    "                    logger.debug(\"Found compatible existing Spark session\")\n",
    "                    return current_session\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "        \n",
    "    def _validate_session(self, spark: SparkSession, mode: str) -> None:\n",
    "        \"\"\"Validate that the Spark session is working correctly.\"\"\"\n",
    "        try:\n",
    "            # Test basic functionality\n",
    "            spark.sql(\"SELECT 1\").collect()\n",
    "            logger.debug(\"Spark session basic functionality validated\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Spark session validation failed: {e}\")\n",
    "            raise SparkSessionError(f\"Session validation failed: {e}\")\n",
    "\n",
    "            \n",
    "    def close_spark_session(self) -> None:\n",
    "        \"\"\"Properly close the Spark session and clean up resources.\"\"\"\n",
    "        if hasattr(self, 'spark') and self.spark:\n",
    "            try:\n",
    "                logger.info(\"Closing Spark session...\")\n",
    "                self.spark.stop()\n",
    "                self.spark = None\n",
    "                logger.info(\"Spark session closed successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error closing Spark session: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - EXTRACT\n",
    "    # ================================================================================================\n",
    "            \n",
    "    def extract(self) -> Self:\n",
    "        \"\"\"\n",
    "        Extract data based on processing mode.\n",
    "        \n",
    "        Bronze: Fetches from NASA NEO API\n",
    "        Silver: Reads from bronze storage bucket  \n",
    "        Gold: Queries silver Iceberg tables\n",
    "        \n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "            \n",
    "        Raises:\n",
    "            DataExtractionError: When extraction fails for any mode\n",
    "            ValueError: When mode is invalid\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting data extraction for mode: {self.mode}\")\n",
    "       \n",
    "        # Select API Client mode\n",
    "        match self.mode:\n",
    "            case 'bronze':    # Extract from source: NASA NEO API request \n",
    "                self._extract_from_api()\n",
    "            \n",
    "            case 'silver':    # Extract from source: neo/bronze/ \n",
    "                self._extract_from_bronze_storage()\n",
    "                \n",
    "            case 'gold':    # Extract from source: neo/silver/\n",
    "                self._extract_from_silver_tables()\n",
    "                \n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid extraction mode: {self.mode}\")\n",
    "\n",
    "        logger.info(f\"Data extraction completed successfully for mode: {self.mode}\")\n",
    "        return self\n",
    "                \n",
    "                \n",
    "    def _extract_from_api(self) -> None:\n",
    "        \"\"\"Extract raw data from NASA NEO API (Bronze mode).\"\"\"\n",
    "        # Generate API request uri\n",
    "        full_uri = f'{self.api_uri}start_date={self.start_date}&end_date={self.end_date}&api_key={self.key}'\n",
    "        \n",
    "        try:\n",
    "            logger.debug(f\"Making API request to: {full_uri}\")\n",
    "            \n",
    "            response = requests.get(full_uri, timeout=5)\n",
    "            response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "\n",
    "            # Convert JSON to bytes\n",
    "            json_bytes = json.dumps(response.json()).encode('utf-8')\n",
    "\n",
    "            # Store data as BytesIO object\n",
    "            self.data = BytesIO(json_bytes)\n",
    "            logger.info(f\"Successfully extracted {len(json_bytes)} bytes from NASA API\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            raise DataExtractionError(f\"API request timed out\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            raise DataExtractionError(\"Failed to connect to NASA API - check network connection\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            raise DataExtractionError(f\"NASA API returned error: {e.response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise DataExtractionError(f\"API request failed: {e}\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise DataExtractionError(\"NASA API returned invalid JSON\")\n",
    "\n",
    "                    \n",
    "    def _extract_from_bronze_storage(self) -> None:\n",
    "        \"\"\"Extract processed data from bronze storage (Silver mode).\"\"\"\n",
    "        logger.debug(\"Extracting data from bronze storage\")\n",
    "        \n",
    "        # Get next item from queue\n",
    "        obj_name = self._update_queue('out')\n",
    "        \n",
    "        if not obj_name:\n",
    "            logger.info(\"No objects available in queue\")\n",
    "            self.data = None\n",
    "            return\n",
    "        \n",
    "        response = None\n",
    "        try:\n",
    "            logger.debug(f\"Retrieving object: {obj_name}\")\n",
    "            response = self.storage.get_object(self.bucket_name, obj_name)\n",
    "            \n",
    "            # Read and parse JSON data\n",
    "            raw_data = response.data.decode('utf-8')\n",
    "            self.data = json.loads(raw_data)\n",
    "            \n",
    "            logger.info(f\"Successfully extracted object '{obj_name}' from bronze storage\")\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            raise DataExtractionError(f\"Invalid JSON in bronze object: {obj_name}\")\n",
    "        except Exception as e:\n",
    "            # Put item back in queue if extraction failed\n",
    "            if obj_name:\n",
    "                self._update_queue('in', obj_name)\n",
    "            raise DataExtractionError(f\"Failed to extract from bronze storage: {e}\")\n",
    "        finally:\n",
    "            # Always clean up HTTP connection\n",
    "            if response:\n",
    "                response.close()\n",
    "                response.release_conn()\n",
    "         \n",
    "        \n",
    "    def _extract_from_silver_tables(self) -> None:\n",
    "        \"\"\"Extract structured data from silver Iceberg tables (Gold mode).\"\"\"\n",
    "        logger.debug(\"Extracting data from silver Iceberg tables\")\n",
    "        \n",
    "        spark = None\n",
    "        try:\n",
    "            # Create Spark session for silver catalog\n",
    "            spark = self.create_spark_session('silver')\n",
    "            \n",
    "            # Execute query\n",
    "            query = \"SELECT * FROM silver_catalog.silver_neo_db.asteroids\"\n",
    "            logger.debug(f\"Executing query: {query}\")\n",
    "            self.data = spark.sql(query)\n",
    "            \n",
    "            # Validate that we got data\n",
    "            if self.data.count() == 0:\n",
    "                logger.warning(\"No data found in silver tables\")\n",
    "            else:\n",
    "                logger.info(f\"Successfully extracted {self.data.count()} records from silver tables\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise DataExtractionError(f\"Failed to extract from silver tables: {e}\")\n",
    "        # Note: Don't close Spark session here - it might be reused\n",
    "        \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - TRANSFORM\n",
    "    # ================================================================================================\n",
    "                     \n",
    "    def transform(self) -> Self:\n",
    "        \"\"\"\n",
    "        Transform data based on processing mode.\n",
    "        \n",
    "        Bronze: Store raw data (no transformation)\n",
    "        Silver: Convert JSON to structured DataFrame\n",
    "        Gold: Prepare analytics-ready dataset\n",
    "\n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "\n",
    "        Raises:\n",
    "            DataTransformationError: When transformation fails\n",
    "            ValueError: When mode is invalid\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting data transformation for mode: {self.mode}\")\n",
    "       \n",
    "        try:\n",
    "            match self.mode:\n",
    "                case 'bronze':\n",
    "                    # Bronze mode: no transformation, data already extracted\n",
    "                    logger.info(\"Bronze mode: No transformation required\")\n",
    "\n",
    "                case 'silver':\n",
    "                    # Silver mode: JSON to structured DataFrame\n",
    "                    if not self.data:\n",
    "                        raise DataTransformationError(\"No data available for silver transformation\")\n",
    "\n",
    "                    if not self.spark:\n",
    "                        self.spark = self.create_spark_session('silver')\n",
    "\n",
    "                    self.data = NeoTransformations.json_to_structured_dataframe(self.data, self.spark)\n",
    "\n",
    "                case 'gold':\n",
    "                    # Gold mode: Analytics-ready dataset\n",
    "                    if not self.data:\n",
    "                        raise DataTransformationError(\"No data available for gold transformation\")\n",
    "\n",
    "                    self.data = NeoTransformations.prepare_analytics_dataset(self.data)\n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid transformation mode: {self.mode}\")\n",
    "\n",
    "            logger.info(f\"Data transformation completed successfully for mode: {self.mode}\")\n",
    "            return self\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data transformation failed for mode {self.mode}: {e}\")\n",
    "            raise DataTransformationError(f\"Transformation failed: {e}\")\n",
    "    \n",
    "    # ================================================================================================\n",
    "    # DATA PROCESSING METHODS - LOAD\n",
    "    # ================================================================================================\n",
    "                              \n",
    "    def load(self) -> Self:\n",
    "        \"\"\"Load data based on processing mode.\"\"\"\n",
    "        logger.info(f\"Starting data load for mode: {self.mode}\")\n",
    "\n",
    "        try:\n",
    "            match self.mode:\n",
    "                case 'bronze':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for bronze load\")\n",
    "\n",
    "                    obj_name = f'{self.mode}/{self.bucket_name}-{self.start_date}_{self.end_date}.json'\n",
    "\n",
    "                    self.storage.put_object(\n",
    "                        self.bucket_name, \n",
    "                        obj_name, \n",
    "                        self.data,\n",
    "                        length=len(self.data.getvalue()),\n",
    "                        content_type='application/json'\n",
    "                    )\n",
    "\n",
    "                    self._update_queue('in', obj_name)\n",
    "                    logger.info(f\"Successfully stored {obj_name} and added to queue\")\n",
    "\n",
    "                case 'silver':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for silver load\")\n",
    "\n",
    "                    database_name = f\"{self.mode}_catalog.{self.mode}_neo_db\"\n",
    "\n",
    "                    self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "                    self.data.writeTo(f'{database_name}.asteroids') \\\n",
    "                             .partitionedBy('observation_date') \\\n",
    "                             .createOrReplace()\n",
    "\n",
    "                    logger.info(f\"Successfully loaded data to silver table\")\n",
    "\n",
    "                case 'gold':\n",
    "                    if not self.data:\n",
    "                        raise DataLoadError(\"No data available for gold load\")\n",
    "\n",
    "                    if not self.spark:\n",
    "                        self.spark = self.create_spark_session('gold')\n",
    "\n",
    "                    database_name = f\"{self.mode}_catalog.{self.mode}_neo_db\"\n",
    "\n",
    "                    self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "                    from pyspark.sql.functions import col\n",
    "                    self.data.writeTo(f'{database_name}.asteroids') \\\n",
    "                             .partitionedBy(col('observation_date')) \\\n",
    "                             .createOrReplace()\n",
    "\n",
    "                    logger.info(f\"Successfully loaded data to gold table\")\n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid load mode: {self.mode}\")\n",
    "\n",
    "            logger.info(f\"Data load completed successfully for mode: {self.mode}\")\n",
    "            return self\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data load failed for mode {self.mode}: {e}\")\n",
    "            raise DataLoadError(f\"Load failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a21e23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:58:03 WARN Utils: Your hostname, newlife resolves to a loopback address: 127.0.1.1; using 192.168.0.196 instead (on interface wlp0s20f3)\n",
      "25/05/26 19:58:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/05/26 19:58:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 19:58:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/05/26 19:58:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/05/26 19:58:13 WARN HadoopTableOperations: Error reading version hint file s3a://neo/silver/silver_neo_db/asteroids/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://neo/silver/silver_neo_db/asteroids/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:358)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:355)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:304)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:121)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/05/26 19:58:13 WARN HadoopTableOperations: Error reading version hint file s3a://neo/silver/silver_neo_db/asteroids/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://neo/silver/silver_neo_db/asteroids/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:84)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:367)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:355)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:304)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:121)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:58:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/05/26 19:58:15 WARN HadoopTableOperations: Error reading version hint file s3a://neo/gold/gold_neo_db/asteroids/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://neo/gold/gold_neo_db/asteroids/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:358)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:355)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:304)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:121)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/05/26 19:58:15 WARN HadoopTableOperations: Error reading version hint file s3a://neo/gold/gold_neo_db/asteroids/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://neo/gold/gold_neo_db/asteroids/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:84)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:367)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:355)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:304)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:121)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 19:58:19 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "for m in ['bronze', 'silver', 'gold']:\n",
    "    # Initialize NeoApiClient\n",
    "    neo_client = NeoApiClient(api_key=api_key_param,\n",
    "                              api_uri=api_uri_param,\n",
    "                              start_date=start_date_param, \n",
    "                              end_date=end_date_param, \n",
    "                              storage=minio_client,  \n",
    "                              bucket_name=bucket_name_param, \n",
    "                              mode=m)\n",
    "\n",
    "    # Execute ETL pipeline task based on mode\n",
    "    neo_client.extract().transform().load()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nasa_spark_kernel",
   "language": "python",
   "name": "nasa_spark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

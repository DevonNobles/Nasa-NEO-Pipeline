{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8aa6334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 02:56:07 WARN Utils: Your hostname, newlife resolves to a loopback address: 127.0.1.1; using 192.168.0.196 instead (on interface wlp0s20f3)\n",
      "25/05/26 02:56:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/05/26 02:56:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 02:56:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing databases in catalog:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "| nasa_neo|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from time import sleep\n",
    "from requests import get\n",
    "import json\n",
    "from collections import deque\n",
    "from io import BytesIO\n",
    "from typing import Self\n",
    "import logging\n",
    "import pickle\n",
    "from transformations import *\n",
    "\n",
    "logging.basicConfig(filename='NeoPipeline.log', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0808ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Minio blob storage\n",
    "def create_minio_client(endpoint='localhost:9000', \n",
    "                        access_key='minioadmin', \n",
    "                        secret_key='minioadmin', \n",
    "                        secure=False):\n",
    "\n",
    "    try: \n",
    "        # Initialize client\n",
    "        client = Minio(endpoint=endpoint, \n",
    "                       access_key=access_key, \n",
    "                       secret_key=secret_key, \n",
    "                       secure=secure)\n",
    "\n",
    "        # verify connection\n",
    "        client.list_buckets()\n",
    "        logger.info(f\"Successfully connected to Minio at {endpoint}\")\n",
    "        connection_status = True\n",
    "\n",
    "        return client, connection_status\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{e}\")\n",
    "        connection_status = False\n",
    "        \n",
    "        return client, connection_status\n",
    "\n",
    "    \n",
    "minio_client, cur_connection_status = create_minio_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a105f68",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set parameters for Airflow\n",
    "api_key_param = 'Sfn0wfG6FG6E3D5Hu8MrxSja38yMXftWqboKv6ZH'\n",
    "api_uri_param = 'https://api.nasa.gov/neo/rest/v1/feed?'\n",
    "start_date_param = '2025-05-02'\n",
    "end_date_param = '2025-05-09'\n",
    "bucket_name_param = 'neo'\n",
    "mode = 'silver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e79434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeoApiClient:\n",
    "    def __init__(self, \n",
    "                 api_key,\n",
    "                 api_uri,\n",
    "                 start_date, \n",
    "                 end_date, \n",
    "                 storage, \n",
    "                 connection_status,\n",
    "                 bucket_name,\n",
    "                 mode):\n",
    "        \n",
    "        self.key = api_key\n",
    "        self.api_uri = api_uri\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.storage = storage\n",
    "        self.connection_status = connection_status\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.data = None\n",
    "\n",
    "    \n",
    "    def _update_queue(self, mode: str, object_name: str=None) -> None:\n",
    "        \n",
    "        # List objects in bucket\n",
    "        bucket_objects = [obj.object_name for obj in self.storage.list_objects(self.bucket_name)]\n",
    "        \n",
    "        # Verify neo/queue exists, create it, if it does not exist\n",
    "        if 'queue' not in bucket_objects:\n",
    "            queue = pickle.dumps(deque([]))\n",
    "            queue_file = BytesIO(queue)\n",
    "            \n",
    "            self.storage.put_object(self.bucket_name,\n",
    "                                    'queue',\n",
    "                                    queue_file,\n",
    "                                    length=len(queue_file.getvalue()))\n",
    "            \n",
    "        # Select update mode\n",
    "        match mode:\n",
    "            case 'in':\n",
    "                HttpResponse = self.storage.get_object(self.bucket_name, 'queue')\n",
    "                queue = pickle.loads(HttpResponse.data)\n",
    "                \n",
    "                queue.append(object_name)\n",
    "                \n",
    "                queue_bytes = pickle.dumps(queue)\n",
    "                queue_file = BytesIO(queue_bytes)\n",
    "\n",
    "                self.storage.put_object(self.bucket_name,\n",
    "                                        'queue',\n",
    "                                        queue_file,\n",
    "                                        length=len(queue_file.getvalue()))               \n",
    "                \n",
    "            case 'out':\n",
    "                HttpResponse = self.storage.get_object(self.bucket_name, 'queue')\n",
    "                queue = pickle.loads(HttpResponse.data)\n",
    "                \n",
    "                if queue:\n",
    "                    item = queue.popleft()\n",
    "                else:\n",
    "                    return None\n",
    "                \n",
    "                queue_bytes = pickle.dumps(queue)\n",
    "                queue_file = BytesIO(queue_bytes)\n",
    "\n",
    "                self.storage.put_object(self.bucket_name,\n",
    "                                        'queue',\n",
    "                                        queue_file,\n",
    "                                        length=len(queue_file.getvalue()))\n",
    "            \n",
    "                return item\n",
    "\n",
    "                \n",
    "    def print_queue(self):\n",
    "        HttpResponse = self.storage.get_object(self.bucket_name, 'queue')\n",
    "        queue = pickle.loads(HttpResponse.data)\n",
    "        print(queue)    \n",
    "    \n",
    "        \n",
    "    def extract(self) -> Self:\n",
    "        logger.info(f'Mode: {self.mode}......extracting')\n",
    "        # Generate API request uri\n",
    "        full_uri = f'{self.api_uri}start_date={self.start_date}&end_date={self.end_date}&api_key={self.key}'\n",
    "       \n",
    "        # Select API Client mode\n",
    "        match self.mode:\n",
    "            \n",
    "            case 'bronze':    # Ingest from source: NASA NEO API request \n",
    "                \n",
    "                try:\n",
    "                    # requests.get()\n",
    "                    HttpResponse = get(full_uri, timeout=5)\n",
    "                    HttpResponse.raise_for_status()  # Raises HTTPError for bad responses\n",
    "    \n",
    "                    # Convert JSON to bytes\n",
    "                    json_bytes = json.dumps(HttpResponse.json()).encode('utf-8')\n",
    "\n",
    "                    # Store data as BytesIO object\n",
    "                    self.data = BytesIO(json_bytes)\n",
    "                    logger.info(f\"JSON data file extracted from {self.api_uri}\")\n",
    "\n",
    "                except requests.exceptions.HTTPError as e:\n",
    "                    logger.error(f\"HTTP Error: {e}\")\n",
    "                    \n",
    "                except requests.exceptions.ConnectionError as e:\n",
    "                    logger.error(f\"Error Connecting: {e}\")\n",
    "                    \n",
    "                except requests.exceptions.Timeout as e:\n",
    "                    logger.error(f\"Timeout Error: {e}\")\n",
    "                    \n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    logger.error(f\"Unknown Error: {e}\")\n",
    "\n",
    "                return self\n",
    "            \n",
    "            case 'silver':    # Ingest from source: neo/bronze/ \n",
    "                \n",
    "                if self.connection_status:\n",
    "\n",
    "                    # Get file name from neo/queue object\n",
    "                    obj_name = self._update_queue('out')\n",
    "\n",
    "                    if obj_name:\n",
    "\n",
    "                        try:\n",
    "                            # Extract binary JSON file from neo/bronze/\n",
    "                            HttpResponse = self.storage.get_object(self.bucket_name, obj_name)\n",
    "\n",
    "                            # Convert JSON to python dictionary\n",
    "                            data: str = HttpResponse.data.decode('utf-8')\n",
    "                            self.data: dict = json.loads(data)\n",
    "                            logger.info(f\"{obj_name} retrieved from '{self.bucket_name}' bucket\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Data extraction from {obj_name} failed: {e}\")\n",
    "\n",
    "                        finally:\n",
    "                            HttpResponse.close()\n",
    "                            HttpResponse.release_conn()\n",
    "\n",
    "                    else:\n",
    "                        logger.info('No objects left in queue')\n",
    "                \n",
    "                else:\n",
    "                    logger.error(\"Connection Error: No files extracted\")\n",
    "                \n",
    "                return self\n",
    "                \n",
    "            case 'gold':    # Ingest from source: neo/silver/\n",
    "                \n",
    "                # Generate neo bucket silver data path\n",
    "                obj_name = f'silver/{self.bucket_name}-{self.start_date}_{self.end_date}.json'\n",
    "                \n",
    "                if self.connection_status:\n",
    "\n",
    "                    try:\n",
    "                        # Extract binary JSON file from neo/bronze/\n",
    "                        HttpResponse = self.storage.get_object(self.bucket_name, obj_name)\n",
    "                        \n",
    "                        # convert JSON to python dictionary\n",
    "                        data: str = HttpResponse.data.decode('utf-8')\n",
    "                        self.data: dict = json.loads(data)\n",
    "                        logger.info(f\"{obj_name} retrieved from '{self.bucket_name}' bucket\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Data extraction from {obj_name} failed: {e}\")\n",
    "\n",
    "                    finally:\n",
    "                        HttpResponse.close()\n",
    "                        HttpResponse.release_conn()\n",
    "                    \n",
    "                    return self\n",
    "                \n",
    "                else:\n",
    "                    logger.error(\"Connection Error: No files extracted\")\n",
    "                    return self\n",
    "\n",
    "                \n",
    "    def transform(self) -> Self:\n",
    "        logger.info(f'Mode: {self.mode}......transforming')\n",
    "        match self.mode:\n",
    "            case 'bronze':\n",
    "                logger.info(\"No bronze transformation implemented\")\n",
    "                return self\n",
    "            \n",
    "            case 'silver':\n",
    "                self.data = json_to_iceberg_table(self.data)\n",
    "                logger.info(\"JSON converted to pyspark DataFrame\")\n",
    "                return self\n",
    "            \n",
    "            case 'gold':\n",
    "                logger.info(\"Final data transformation complete\")\n",
    "                return self\n",
    "                \n",
    "                        \n",
    "    def load(self) -> None:\n",
    "        logger.info(f'Mode: {self.mode}......loading')\n",
    "        # Select API Client mode\n",
    "        match self.mode:\n",
    "                                    \n",
    "            case 'bronze':\n",
    "            \n",
    "                # Generate neo bucket bronze data path                   \n",
    "                obj_name = f'{self.mode}/{self.bucket_name}-{self.start_date}_{self.end_date}.json'\n",
    "\n",
    "                if self.connection_status:\n",
    "                   \n",
    "                    try:\n",
    "                        # Put client data in neo/bronze bucket\n",
    "                        self.storage.put_object(\n",
    "                            self.bucket_name, \n",
    "                            obj_name, \n",
    "                            self.data,\n",
    "                            length=len(self.data.getvalue()),\n",
    "                            content_type='application/json'\n",
    "                        )\n",
    "                                    \n",
    "                        # Add file name to neo/queue object\n",
    "                        self._update_queue('in', obj_name)\n",
    "\n",
    "                        logger.info(\n",
    "                            f'JSON file successfully uploaded as {obj_name} in bucket {self.bucket_name}'\n",
    "                        )\n",
    "                                    \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Load Error: No files loaded: {e}\")\n",
    "                                    \n",
    "                else:\n",
    "                    logger.error(\"Connection Error: No files loaded\")\n",
    "            \n",
    "            case 'silver':\n",
    "                self.data.writeTo('myminio.nasa_neo.asteroids').partitionedBy(col('observation_date')).createOrReplace()\n",
    "                logger.info(\n",
    "                    f'Updated asteroids table in the nasa_neo database at {mode}/ in bucket {self.bucket_name}'\n",
    "                )\n",
    "\n",
    "            case 'gold':\n",
    "                obj_name = f'{self.mode}/{self.bucket_name}-{self.start_date}_{self.end_date}.parquet'\n",
    "                logger.info(\n",
    "                    f'Parquet file and Iceberg catalog successfully uploaded, as {obj_name}/ in bucket {self.bucket_name}'\n",
    "                )\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a21e23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque(['bronze/neo-2025-05-02_2025-05-09.json'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 02:56:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 02:56:17 WARN HadoopTableOperations: Error reading version hint file s3a://neo/silver/nasa_neo/asteroids/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://neo/silver/nasa_neo/asteroids/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:358)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:355)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:304)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:121)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/05/26 02:56:17 WARN HadoopTableOperations: Error reading version hint file s3a://neo/silver/nasa_neo/asteroids/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://neo/silver/nasa_neo/asteroids/metadata/version-hint.text\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:84)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:367)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:355)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:304)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:121)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 02:56:20 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "for m in ['bronze', 'silver']:\n",
    "    # Initialize NeoApiClient\n",
    "    neo_client = NeoApiClient(api_key=api_key_param,\n",
    "                              api_uri=api_uri_param,\n",
    "                              start_date=start_date_param, \n",
    "                              end_date=end_date_param, \n",
    "                              storage=minio_client, \n",
    "                              connection_status=cur_connection_status, \n",
    "                              bucket_name=bucket_name_param, \n",
    "                              mode=m)\n",
    "\n",
    "    # Execute ETL pipeline task based on mode\n",
    "    neo_client.extract().transform().load()\n",
    "    neo_client.print_queue()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nasa_spark_kernel",
   "language": "python",
   "name": "nasa_spark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
